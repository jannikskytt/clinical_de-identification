{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">2. Create validation and test set</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import copy\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Show all outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Progress bar\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/work/NER/all_journals_reducedcols_wordcount_nodupna_NER_split.txt\",dtype='str',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['count'] = data['count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['ID'] = data['JournalNote_ID']+'_'+data['split_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.loc[(data['count']>=8) & (data['count']<=70),['split','ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = ['Samples','ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = data.sample(n=150000, replace=False, random_state=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read identifiers output from \"1_create_identifier_list.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read identifiers\n",
    "entities = pd.read_csv('all_tags.txt', index_col = 'entity', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities['tag'] = entities['tag'].progress_apply(lambda x: x[1:-1].replace(\"'\",\"\").replace(\" \",\"\").split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count occurrence of identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities_dict = entities.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trie():\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n",
    "    \n",
    "    Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n",
    "    The corresponding Regex should match much faster than a simple Regex union.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def add(self, word):\n",
    "        ref = self.data\n",
    "        for char in word:\n",
    "            ref[char] = char in ref and ref[char] or {}\n",
    "            ref = ref[char]\n",
    "        ref[''] = 1\n",
    "\n",
    "    def dump(self):\n",
    "        return self.data\n",
    "\n",
    "    def quote(self, char):\n",
    "        return re.escape(char)\n",
    "\n",
    "    def _pattern(self, pData):\n",
    "        data = pData\n",
    "        if \"\" in data and len(data.keys()) == 1:\n",
    "            return None\n",
    "\n",
    "        alt = []\n",
    "        cc = []\n",
    "        q = 0\n",
    "        for char in sorted(data.keys()):\n",
    "            if isinstance(data[char], dict):\n",
    "                try:\n",
    "                    recurse = self._pattern(data[char])\n",
    "                    alt.append(self.quote(char) + recurse)\n",
    "                except:\n",
    "                    cc.append(self.quote(char))\n",
    "            else:\n",
    "                q = 1\n",
    "        cconly = not len(alt) > 0\n",
    "\n",
    "        if len(cc) > 0:\n",
    "            if len(cc) == 1:\n",
    "                alt.append(cc[0])\n",
    "            else:\n",
    "                alt.append('[' + ''.join(cc) + ']')\n",
    "\n",
    "        if len(alt) == 1:\n",
    "            result = alt[0]\n",
    "        else:\n",
    "            result = \"(?:\" + \"|\".join(alt) + \")\"\n",
    "\n",
    "        if q:\n",
    "            if cconly:\n",
    "                result += \"?\"\n",
    "            else:\n",
    "                result = \"(?:%s)?\" % result\n",
    "        return result\n",
    "\n",
    "    def pattern(self):\n",
    "        return self._pattern(self.dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoding: utf-8\n",
    "trie = Trie()\n",
    "for key in tqdm(entities_dict.keys()):\n",
    "    trie.add(key)\n",
    "regex= re.compile(r\"(?<!\\w)\" + trie.pattern() + r\"(?:(?:(?<![szx])(?:(?!\\w)|(?=s(?!\\w))))|(?:(?<=[szx])(?!\\w)))\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_dict = {}\n",
    "for sample in tqdm(text['Samples']):\n",
    "    for match in re.finditer(regex, sample):\n",
    "        s = match.start()\n",
    "        e = match.end()\n",
    "        ent = sample[s:e].lower()\n",
    "        if ent in found_dict:\n",
    "            found_dict[ent]['found']+=1\n",
    "        else:\n",
    "            found_dict[ent]={}\n",
    "            found_dict[ent]['found']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate rates in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in entities_dict:\n",
    "    if key in found_dict:\n",
    "        found_dict[key]['tag'] = entities_dict[key]['tag']\n",
    "        found_dict[key]['prob_pop'] = entities_dict[key]['prob_pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_frame = pd.DataFrame.from_dict(found_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_frame['prob_sample'] = found_frame['found']/len(text)\n",
    "found_frame.drop(labels='found', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_frame['sample:pop ratio'] = found_frame['prob_sample']/found_frame['prob_pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_frame.sort_values(by='sample:pop ratio', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_frame.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate median rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# median ratio\n",
    "found_frame.loc[:,'sample:pop ratio'].median() #combi\n",
    "found_frame.loc[found_frame['tag'].str.len()==1,'sample:pop ratio'].median() #single\n",
    "found_frame.loc[found_frame['tag'].str.len()>1,'sample:pop ratio'].median() #ambi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set ratio floor and ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_roof = 5.180362222222222 #median\n",
    "\n",
    "filtered_frame_single = found_frame[found_frame['tag'].str.len()==1]\n",
    "filtered_frame_single = filtered_frame_single.loc[filtered_frame_single['sample:pop ratio']<=single_roof,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_frame_single.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ambi_floor = 1\n",
    "ambi_roof = 5.180362222222222 #median\n",
    "\n",
    "filtered_frame_ambi = found_frame[found_frame['tag'].str.len()>1]\n",
    "filtered_frame_ambi = filtered_frame_ambi.loc[filtered_frame_ambi['sample:pop ratio']<=ambi_roof,:]\n",
    "filtered_frame_ambi.loc[(filtered_frame_ambi['sample:pop ratio']<ambi_floor),'tag'] = filtered_frame_ambi.loc[(filtered_frame_ambi['sample:pop ratio']<ambi_floor),'tag'].progress_apply(lambda x: [item for item in x if item not in ['WORDS','ABB','DRUGS','SNOMED','SKS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_frame_ambi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_frame_ambi.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_dict = {}\n",
    "for key in filtered_frame_ambi.index:\n",
    "    filtered_dict[key] = filtered_frame_ambi.loc[key,'tag']\n",
    "for key in filtered_frame_single.index:\n",
    "    filtered_dict[key] = filtered_frame_single.loc[key,'tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoding: utf-8\n",
    "trie = Trie()\n",
    "for key in tqdm(filtered_dict.keys()):\n",
    "    trie.add(key)\n",
    "regex= re.compile(r\"(?<!\\w)\" + trie.pattern() + r\"(?:(?:(?<![szx])(?:(?!\\w)|(?=s(?!\\w))))|(?:(?<=[szx])(?!\\w)))\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text['chars'] = text['Samples'].progress_apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text['entity'] = text['chars'].progress_apply(lambda x: [False]*len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text['split_index'] = text['Samples'].progress_apply(lambda x: [match.start() for match in re.finditer('[^\\w,.-]|(?<=\\d)-(?=\\d)|[.,](?!\\d)|(?<=\\w)-\\s', x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_split_function(split_index, chars):\n",
    "    word_split = [False]*chars\n",
    "    for split in split_index: word_split[split]=True\n",
    "    return word_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text['word_split'] = text.progress_apply(lambda x: word_split_function(x['split_index'],len(x['chars'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index in tqdm(text.index.to_list()):\n",
    "    sample = text.loc[index,'Samples']\n",
    "    for match in re.finditer(regex, sample):\n",
    "        s = match.start()\n",
    "        e = match.end()\n",
    "        ent = sample[s:e].lower()\n",
    "        text.loc[index,'entity'][s:e] = [filtered_dict[ent]]*(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make sentences and ner vector\n",
    "def find_most_frequent(ent_list):\n",
    "    '''\n",
    "    Returns the most frequent element in the list.\n",
    "    '''\n",
    "    #print(ent_list)\n",
    "    #If there are no entities for that word, we return False\n",
    "    if len(ent_list)==0:\n",
    "        return False\n",
    "    \n",
    "    ent=''\n",
    "    \n",
    "    if len(list(set(tuple(i) for i in ent_list)))==1:\n",
    "        ent = ent_list[0]\n",
    "        #print('returning the only one',ent)\n",
    "    else:\n",
    "        ent = max(set(tuple(i) for i in ent_list), key = ent_list.count)\n",
    "        #print('returning the max',ent)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text['sentence'] = ''\n",
    "text['sentence_ent'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index in tqdm(text.index.to_list()):\n",
    "    #############\n",
    "    sentence = []\n",
    "    word = []\n",
    "    #############\n",
    "    sentence_ent = []\n",
    "    word_ent = []\n",
    "    #############\n",
    "\n",
    "    chars = text.loc[index,'chars']\n",
    "    char_entity = text.loc[index,'entity']\n",
    "    for i in range(len(chars)): #go through chars\n",
    "        if text.loc[index,'word_split'][i]: #if word split\n",
    "            if len(word)>0:\n",
    "                sentence.append(\"\".join([c for c in word if not c.isspace()]))\n",
    "                sentence_ent.append(find_most_frequent(word_ent)) #return word entity or False\n",
    "            if not chars[i].isspace():\n",
    "                sentence.append(chars[i])\n",
    "                sentence_ent.append(False if char_entity[i]==False else char_entity[i])\n",
    "            word = []\n",
    "            word_ent = []\n",
    "        else: #No split\n",
    "            word.append(chars[i]) #Append char to word\n",
    "            if char_entity[i]!=False:\n",
    "                word_ent.append(char_entity[i]) #Append char entity to word_ent if exist\n",
    "    text.at[index,'sentence'] = sentence\n",
    "    text.at[index,'sentence_ent'] = sentence_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text['sentence_ent_reformat'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index in tqdm(text.index.to_list()): #sentence in sentences\n",
    "    sentence_ent_refomat =  []\n",
    "    prior = []\n",
    "    temp_ent = []\n",
    "    sentence_ent = text.loc[index,'sentence_ent'] #temp\n",
    "    for t in range(len(sentence_ent)): #tag in sentence\n",
    "        if sentence_ent[t]!=False: #if there is a tag\n",
    "            if sentence_ent[t]!=prior: #if tag is not equal to prior\n",
    "                if len(temp_ent)!=0: sentence_ent_refomat.append(temp_ent) #then we have a finished tag...\n",
    "                temp_ent = [t,t,sentence_ent[t]] #and we start a new\n",
    "                prior = sentence_ent[t]\n",
    "            else: #if tag is equal to prior\n",
    "                temp_ent[1] = t #Update end tag\n",
    "        else: #if there is not a tag\n",
    "            if len(temp_ent)!=0:\n",
    "                sentence_ent_refomat.append(temp_ent)\n",
    "                temp_ent = [] #reset temp\n",
    "                prior = [] #reset prior\n",
    "    if len(temp_ent)!=0: sentence_ent_refomat.append(temp_ent)\n",
    "    text.at[index,'sentence_ent_reformat'] = sentence_ent_refomat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def next_to_same(tags, tag):\n",
    "    '''\n",
    "    Takes a list of tags and an index of an ambiguous tag to check in that list.\n",
    "    Checks if the ambiguous tag is next to a single tag and has that tag. If so, it returns that tag.\n",
    "    '''\n",
    "    \n",
    "    #check left\n",
    "    left=False\n",
    "    left_tag = []\n",
    "    if tag-1>=0: #if exist entity somewhere left of\n",
    "        if (tags[tag][0])-(tags[tag-1][1])==1: #if they are neighbors\n",
    "            if len(tags[tag-1][2])==1: #if left neigbor only has one tag\n",
    "                if tags[tag-1][2][0] in tags[tag][2]: #if center also has that tag\n",
    "                    left = True\n",
    "                    left_tag = tags[tag-1][2]\n",
    "            \n",
    "    #check right\n",
    "    right=False\n",
    "    right_tag = []\n",
    "    if tag+1<len(tags): #if exist entity somewhere right of\n",
    "        if (tags[tag+1][1])-(tags[tag][0])==1: #if they are neighbors\n",
    "            if len(tags[tag+1][2])==1: #if right neigbor only has one tag\n",
    "                if tags[tag+1][2][0] in tags[tag][2]: #if center also has that tag\n",
    "                    right = True\n",
    "                    right_tag = tags[tag+1][2]    \n",
    "    if left==False and right==False:\n",
    "        #print('left==False and right==False')\n",
    "        return tags[tag][2]\n",
    "    if left==False and right==True:\n",
    "        #print('left==False and right==True')\n",
    "        return right_tag\n",
    "    if left==True and right==False:\n",
    "        #print('left==True and right==False')\n",
    "        return left_tag\n",
    "    if left==True and right==True:\n",
    "        #print('left==True and right==True...')\n",
    "        if left_tag == right_tag:\n",
    "            #print('...left_tag')\n",
    "            return left_tag\n",
    "        else:\n",
    "            #print('...tags[tag][2]')\n",
    "            return tags[tag][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_tags(sentence, before, after,file):\n",
    "    \n",
    "    if len(before)==0 or len(after)==0: #In case of just printing sentence for no-tag sentences\n",
    "        with open(file,'a',encoding='utf8') as file:\n",
    "            file.write('sentence: '+' '.join(sentence)+'\\n\\n\\n')\n",
    "    else:\n",
    "        #Print before\n",
    "        tag_per_word_before = [' ']*before[0][0] #start\n",
    "        for tag in range(len(before)):\n",
    "            tag_per_word_before += [before[tag][2]]+['->']*(before[tag][1]-before[tag][0])\n",
    "            if tag+1!=len(before): tag_per_word_before += [' ']*(before[tag+1][0]-before[tag][1]-1)\n",
    "        tag_per_word_before+= [' ']*(len(sentence)-before[-1][1]-1) #end\n",
    "\n",
    "        #Print after\n",
    "        tag_per_word_after = [' ']*after[0][0] #start\n",
    "        for tag in range(len(after)):\n",
    "            tag_per_word_after += [after[tag][2]]+['->']*(after[tag][1]-after[tag][0])\n",
    "            if tag+1!=len(after): tag_per_word_after += [' ']*(after[tag+1][0]-after[tag][1]-1)\n",
    "        tag_per_word_after+= [' ']*(len(sentence)-after[-1][1]-1) #end\n",
    "\n",
    "        line_one=  'sentence: '\n",
    "        line_two=  'before:   '\n",
    "        line_three='after:    '\n",
    "\n",
    "        for s,b,a in zip(sentence,tag_per_word_before,tag_per_word_after):\n",
    "            #convert to string if list\n",
    "            b=str(b)\n",
    "            a=str(a)\n",
    "\n",
    "            max_len = max(len(s),len(b),len(a))\n",
    "            line_one+=(s.center(max_len)+' ')\n",
    "            line_two+=(b.center(max_len)+' ')\n",
    "            line_three+=(a.center(max_len)+' ')\n",
    "\n",
    "        with open(file,'a',encoding='utf8') as file:\n",
    "            file.write(line_one+'\\n')\n",
    "            file.write(line_two+'\\n')\n",
    "            file.write(line_three+'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#If an ambiguous tag is next to a single tag and has that tag -> set to that tag.\n",
    "#Reset txt file\n",
    "file = open(\"visualization_next_to_same.txt\",\"w\")\n",
    "file.close()\n",
    "\n",
    "count = 0\n",
    "for index in tqdm(text.index.to_list()): #sentence in sentences\n",
    "    tags = copy.deepcopy(text['sentence_ent_reformat'].at[index])\n",
    "    old_tags = copy.deepcopy(tags)\n",
    "    changed_something = False #for printing\n",
    "    if len(tags)>0: #if there is some tag(s)\n",
    "        for tag in range(len(tags)): #for each tag\n",
    "            current_tag = tag\n",
    "            try_change = True\n",
    "            while try_change==True:\n",
    "                if len(tags[current_tag][2])>1: #if ambi tag\n",
    "                    tags[current_tag][2] = copy.deepcopy(next_to_same(tags,current_tag)) #give same tag as un-ambi neigbor(s) if exists\n",
    "                    if len(tags[current_tag][2])==1: #if it changed\n",
    "                        changed_something = True #print\n",
    "                        if current_tag>0:\n",
    "                            current_tag = current_tag-1 #Then rewind 1 to see if the new tag has changed anything for the previous tag.\n",
    "                        else:\n",
    "                            try_change=False #stop rewinding when we reach start of list\n",
    "                    else:\n",
    "                        try_change=False #stop rewinding when change did not happen\n",
    "                else:\n",
    "                    try_change=False #stop rewinding when we meet single tag\n",
    "        if changed_something == True:\n",
    "            print_tags(text.loc[index,'sentence'], old_tags, tags,file='visualization_next_to_same.txt')\n",
    "            text.at[index,'sentence_ent_reformat']=copy.deepcopy(tags)\n",
    "            count+=1\n",
    "print(count,'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#If a middle initial is between two name tags, tag it with a name tag\n",
    "#Reset txt file\n",
    "file = open(\"visualization_middle_initial.txt\",\"w\")\n",
    "file.close()\n",
    "\n",
    "#For each sample\n",
    "#Check if there exists 2 single name tags that have one of the following between them in sentence:\n",
    "# w .|W|w . w .|ww\n",
    "\n",
    "#maybe:\n",
    "#also check if a single name tag is preceded or followed by one of the following\n",
    "#W|w . -> be more careful here\n",
    "\n",
    "count = 0\n",
    "for index in tqdm(text.index.to_list()): #sentence in sentences\n",
    "    sentence = copy.deepcopy(text['sentence'].at[index])\n",
    "    tags = copy.deepcopy(text['sentence_ent_reformat'].at[index])\n",
    "    old_tags = copy.deepcopy(tags)\n",
    "    tags_to_insert=[]\n",
    "    count_tags = 0\n",
    "    if len(tags)>1: #if there is MORE THAN ONE TAG\n",
    "        for tag in range(len(tags)-1): #for each tag (exluding last)\n",
    "            if tags[tag][2]==['NAME'] and tags[tag+1][2]==['NAME']: #Check if there are two single name tags next to each other\n",
    "                start=tags[tag][1]+1\n",
    "                end=tags[tag+1][0]\n",
    "                middle = sentence[start:end] #find middle\n",
    "                match=False\n",
    "                if len(middle)>0 and len(middle)<=4: #check if elegible for being initial\n",
    "                    if len(middle)==1:\n",
    "                        #W\n",
    "                        #WW                    \n",
    "                        regex = re.compile(r'^[A-ZÆØÅ]{1,2}$')\n",
    "                        if regex.search(middle[0]):\n",
    "                            match=True\n",
    "                    elif len(middle)==2:\n",
    "                        #w .                    \n",
    "                        regex = re.compile(r'^[a-zæøåA-ZÆØÅ]{1}$')\n",
    "                        if middle[1]=='.' and regex.search(middle[0]):\n",
    "                            match=True\n",
    "                    elif len(middle)==4:\n",
    "                        #w . w .                    \n",
    "                        regex = re.compile(r'^[a-zæøåA-ZÆØÅ]{1}$')\n",
    "                        if middle[1]=='.' and middle[3]=='.' and regex.search(middle[0]) and regex.search(middle[2]):\n",
    "                            match=True\n",
    "                if match==True:\n",
    "                    insert_tag = [start,end-1,['NAME']]\n",
    "                    tags_to_insert.append([tag+1+count_tags,insert_tag])\n",
    "                    count_tags+=1 #we count have many tags we append so that we can adjust index when inserting\n",
    "    for i,tag in tags_to_insert:\n",
    "        tags.insert(i,tag)\n",
    "    if len(tags_to_insert)>0:\n",
    "        print_tags(text.loc[index,'sentence'], old_tags, tags,file=\"visualization_middle_initial.txt\")\n",
    "        text.at[index,'sentence_ent_reformat']=copy.deepcopy(tags)\n",
    "        count+=1 #for stats\n",
    "print(count,'fixed')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Merge similar single tags that are next to each other.\n",
    "#Reset txt file\n",
    "file = open(\"visualization_merge.txt\",\"w\")\n",
    "file.close()\n",
    "\n",
    "count = 0\n",
    "for index in tqdm(text.index.to_list()): #sentence in sentences\n",
    "    sentence = copy.deepcopy(text['sentence'].at[index])\n",
    "    tags = copy.deepcopy(text['sentence_ent_reformat'].at[index])\n",
    "    old_tags = copy.deepcopy(tags)\n",
    "    changed_something = False #for printing\n",
    "    if len(tags)>1: #if there is MORE THAN ONE TAG\n",
    "        tag=0\n",
    "        while tag<len(tags)-1: #while exist tag and right neighbor\n",
    "            #print(tag)\n",
    "            #remove two tags and insert merge if neighbors - dont change index\n",
    "            neighbors = tags[tag][1]+1==tags[tag+1][0] #neighbors\n",
    "            single = len(tags[tag][2])==1 and len(tags[tag+1][2])==1 #single tags\n",
    "            same = tags[tag][2]==tags[tag+1][2] #same tags\n",
    "            #print('neighbors', 'single', 'same',neighbors, single, same)\n",
    "            if neighbors and single and same:\n",
    "                merge = [tags[tag][0],tags[tag+1][1],tags[tag][2]]\n",
    "                tags.insert(tag+2,merge) #insert merge\n",
    "                del tags[tag:tag+2] #delete the merged tags\n",
    "                changed_something=True\n",
    "            else:\n",
    "                #change index if no neighbors\n",
    "                tag+=1\n",
    "    if changed_something:\n",
    "        print_tags(text.loc[index,'sentence'], old_tags, tags,file=\"visualization_merge.txt\")\n",
    "        text.at[index,'sentence_ent_reformat']=copy.deepcopy(tags)\n",
    "        count+=1\n",
    "print(count,'fixed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print sentences with tags that are single, ambi, and empty to check for further processing\n",
    "# Do statistics\n",
    "#Reset txt file\n",
    "file = open(\"visualization_single_tags.txt\",\"w\")\n",
    "file.close()\n",
    "file = open(\"visualization_ambi_tags.txt\",\"w\")\n",
    "file.close()\n",
    "file = open(\"visualization_no_tags.txt\",\"w\")\n",
    "file.close()\n",
    "\n",
    "# For counting and histogram\n",
    "single_count = []\n",
    "ambi_count = []\n",
    "no_count = []\n",
    "single_tag_dict = {'NAME': 0, 'STREET': 0, 'CITY': 0}\n",
    "sentence_single_tag_dict = {'NAME': 0, 'STREET': 0, 'CITY': 0}\n",
    "\n",
    "for index in tqdm(text.index.to_list()): #sentence in sentences\n",
    "    sentence = copy.deepcopy(text['sentence'].at[index])\n",
    "    tags = copy.deepcopy(text['sentence_ent_reformat'].at[index])\n",
    "    ambi_tag = False #for printing\n",
    "    no_tag = False\n",
    "    if len(tags)==0:\n",
    "        no_tag=True #no tags\n",
    "    else: #if there is ONE OR MORE\n",
    "        for tag in tags:\n",
    "            if len(tag[2])>1:\n",
    "                ambi_tag=True\n",
    "    if no_tag:\n",
    "        no_count.append(len(sentence))\n",
    "        print_tags(text.loc[index,'sentence'], tags, tags,file=\"visualization_no_tags.txt\") #prints the two same lines\n",
    "    elif ambi_tag:\n",
    "        ambi_count.append(len(sentence))\n",
    "        print_tags(text.loc[index,'sentence'], tags, tags,file=\"visualization_ambi_tags.txt\") #prints the two same lines\n",
    "    else: #single tags only\n",
    "        single_count.append(len(sentence))\n",
    "        print_tags(text.loc[index,'sentence'], tags, tags,file=\"visualization_single_tags.txt\") #prints the two same lines\n",
    "        \n",
    "        name_in_sentence=0\n",
    "        city_in_sentence=0\n",
    "        street_in_sentence=0\n",
    "        #total distribution of tags\n",
    "        for b,e,t in tags:\n",
    "            single_tag_dict[t[0]]+=1\n",
    "            if t[0]=='NAME': name_in_sentence=1\n",
    "            if t[0]=='CITY': city_in_sentence=1\n",
    "            if t[0]=='STREET': street_in_sentence=1\n",
    "        \n",
    "        #distribution of sentences containing tags\n",
    "        sentence_single_tag_dict['NAME']+=name_in_sentence\n",
    "        sentence_single_tag_dict['CITY']+=city_in_sentence\n",
    "        sentence_single_tag_dict['STREET']+=street_in_sentence\n",
    "\n",
    "print(len(single_count),'single-tag samples')\n",
    "print(len(ambi_count),'ambi-tag samples')\n",
    "print(len(no_count),'no-tag samples')\n",
    "print('Total distribution of tags that occur in single-tag samples:')\n",
    "print(single_tag_dict)\n",
    "print('Number of single-tag samples with tag:')\n",
    "print(sentence_single_tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plotting histogram\n",
    "max_len = max(max(single_count),max(ambi_count),max(no_count))\n",
    "print(max_len)\n",
    "\n",
    "def plot_hist(data, max_len):\n",
    "    bins = np.linspace(0, \n",
    "                       max_len,\n",
    "                       20) # fixed number of bins\n",
    "\n",
    "    plt.xlim([0, max_len])\n",
    "\n",
    "    plt.hist(data, bins=bins, alpha=0.5)\n",
    "    plt.title('Histogram (20 bins)')\n",
    "    plt.xlabel('Length (20 evenly spaced bins)')\n",
    "    plt.ylabel('count')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(single_count)\n",
    "max(ambi_count)\n",
    "max(no_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_hist(single_count, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_hist(ambi_count, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_hist(no_count, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate samples and create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Give sentence label 0 if empty, 1 if single, 2 if ambi\n",
    "def sentence_label(tags):\n",
    "    if len(tags)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        for b,e,t in tags:\n",
    "            if len(t)>1:\n",
    "                return 2 #if encounter ambi\n",
    "        return 1 #if we did not encounter ambi\n",
    "\n",
    "text['label'] = text['sentence_ent_reformat'].progress_apply(lambda x: sentence_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_tags = text.loc[text['label']==1,['ID','sentence','sentence_ent_reformat']]\n",
    "single_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we take 500 single, 500 ambi, 500 no\n",
    "print('total:',len(single_tags))\n",
    "print('500 samples quartile:',500/len(single_tags))\n",
    "print('1000 samples quartile:',1000/len(single_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tag(tags, tag):\n",
    "    count=0\n",
    "    for b,e,t in tags:\n",
    "        if t[0]==tag:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "seeds = []\n",
    "dists = []\n",
    "\n",
    "\n",
    "while len(seeds)<=20:\n",
    "    \n",
    "    # copy matrix\n",
    "    single_tags_shuffle = single_tags.copy(deep=True)\n",
    "    \n",
    "    #Could be done only once\n",
    "    single_tags_shuffle['NAME count'] = single_tags_shuffle['sentence_ent_reformat'].apply(lambda x: count_tag(x, 'NAME'))\n",
    "    single_tags_shuffle['STREET count'] = single_tags_shuffle['sentence_ent_reformat'].apply(lambda x: count_tag(x, 'STREET'))\n",
    "    single_tags_shuffle['CITY count'] = single_tags_shuffle['sentence_ent_reformat'].apply(lambda x: count_tag(x, 'CITY'))\n",
    "    \n",
    "    \n",
    "    # TRY DIFFERENT RANDOM STATES\n",
    "    seed = random.randint(0,9999)\n",
    "    single_tags_shuffle = single_tags_shuffle.sample(frac=1.0, replace=False,  random_state=seed)\n",
    "\n",
    "    single_tags_shuffle['NAME cumsum'] = single_tags_shuffle['NAME count'].cumsum()\n",
    "    single_tags_shuffle['STREET cumsum'] = single_tags_shuffle['STREET count'].cumsum()\n",
    "    single_tags_shuffle['CITY cumsum'] = single_tags_shuffle['CITY count'].cumsum()\n",
    "\n",
    "\n",
    "    #convert to percentage of total tags\n",
    "    single_tags_shuffle['NAME norm'] = single_tags_shuffle['NAME cumsum']/list(single_tags_shuffle['NAME cumsum'])[-1]\n",
    "    single_tags_shuffle['STREET norm'] = single_tags_shuffle['STREET cumsum']/list(single_tags_shuffle['STREET cumsum'])[-1]\n",
    "    single_tags_shuffle['CITY norm'] = single_tags_shuffle['CITY cumsum']/list(single_tags_shuffle['CITY cumsum'])[-1]\n",
    "\n",
    "\n",
    "    #max_80 = max(len(single_tags_shuffle.query('`NAME norm` <= 0.80')),len(single_tags_shuffle.query('`STREET norm` <= 0.80')),len(single_tags_shuffle.query('`CITY norm` <= 0.80')))\n",
    "    #min_80 = min(len(single_tags_shuffle.query('`NAME norm` <= 0.80')),len(single_tags_shuffle.query('`STREET norm` <= 0.80')),len(single_tags_shuffle.query('`CITY norm` <= 0.80')))\n",
    "    #max_90 = max(len(single_tags_shuffle.query('`NAME norm` <= 0.90')),len(single_tags_shuffle.query('`STREET norm` <= 0.90')),len(single_tags_shuffle.query('`CITY norm` <= 0.90')))\n",
    "    #min_90 = min(len(single_tags_shuffle.query('`NAME norm` <= 0.90')),len(single_tags_shuffle.query('`STREET norm` <= 0.90')),len(single_tags_shuffle.query('`CITY norm` <= 0.90')))\n",
    "    \n",
    "    max_500 = max(len(single_tags_shuffle.query('`NAME norm` <= 0.022626482034573264')),len(single_tags_shuffle.query('`STREET norm` <= 0.022626482034573264')),len(single_tags_shuffle.query('`CITY norm` <= 0.022626482034573264')))\n",
    "    min_500 = min(len(single_tags_shuffle.query('`NAME norm` <= 0.022626482034573264')),len(single_tags_shuffle.query('`STREET norm` <= 0.022626482034573264')),len(single_tags_shuffle.query('`CITY norm` <= 0.022626482034573264')))\n",
    "    \n",
    "    max_1000 = max(len(single_tags_shuffle.query('`NAME norm` <= 0.04533708119871243')),len(single_tags_shuffle.query('`STREET norm` <= 0.04533708119871243')),len(single_tags_shuffle.query('`CITY norm` <= 0.04533708119871243')))\n",
    "    min_1000 = min(len(single_tags_shuffle.query('`NAME norm` <= 0.04533708119871243')),len(single_tags_shuffle.query('`STREET norm` <= 0.04533708119871243')),len(single_tags_shuffle.query('`CITY norm` <= 0.04533708119871243')))\n",
    "    \n",
    "    \n",
    "    dist = (max_500-min_500)+(max_1000-min_1000)\n",
    "    #dist = (max_2-min_2)\n",
    "    \n",
    "    dists.append(dist)\n",
    "    seeds.append(seed)\n",
    "\n",
    "min_index = np.argmin(dists)\n",
    "min_seed = seeds[min_index]\n",
    "for i in range(len(seeds)):\n",
    "    print(seeds[i],':',dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose best labeleling sample\n",
    "single_tags_shuffle = single_tags.copy(deep=True)\n",
    "single_tags_shuffle = single_tags_shuffle.sample(frac=1.0, replace=False,  random_state=min_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label single tags\n",
    "val_test_count = 500 #math.ceil(len(single_tags_shuffle)*0.022626482034573264)\n",
    "train_count = len(single_tags_shuffle)-2*val_test_count\n",
    "\n",
    "split = train_count*['train']+val_test_count*['val']+val_test_count*['test']\n",
    "\n",
    "single_tags_shuffle['set'] = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_tags_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_tags_shuffle.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_tags_shuffle.set.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_tags = text.loc[text['label']==0,['ID','sentence','sentence_ent_reformat']]\n",
    "no_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_tags_shuffle = no_tags.sample(n=1000, replace=False).copy(deep=True)\n",
    "no_tags_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label single tags\n",
    "split = 500*['val']+500*['test']\n",
    "\n",
    "no_tags_shuffle['set'] = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_tags_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_tags_shuffle.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_tags_shuffle.set.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguous tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ambi_tags = text.loc[text['label']==2,['ID','sentence','sentence_ent_reformat']]\n",
    "ambi_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we take 500 single, 500 ambi, 500 no\n",
    "print('total:',len(ambi_tags))\n",
    "print('500 samples quartile:',500/len(ambi_tags))\n",
    "print('1000 samples quartile:',1000/len(ambi_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "500/0.027289597205545246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tag(tags, tag):\n",
    "    count=0\n",
    "    for b,e,t in tags:\n",
    "        if tag in t[0]:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "seeds = []\n",
    "dists = []\n",
    "\n",
    "\n",
    "while len(seeds)<=20:\n",
    "    \n",
    "    # copy matrix\n",
    "    ambi_tags_shuffle = ambi_tags.copy(deep=True)\n",
    "    \n",
    "    #Could be done only once\n",
    "    ambi_tags_shuffle['NAME count'] = ambi_tags_shuffle['sentence_ent_reformat'].apply(lambda x: count_tag(x, 'NAME'))\n",
    "    ambi_tags_shuffle['STREET count'] = ambi_tags_shuffle['sentence_ent_reformat'].apply(lambda x: count_tag(x, 'STREET'))\n",
    "    ambi_tags_shuffle['CITY count'] = ambi_tags_shuffle['sentence_ent_reformat'].apply(lambda x: count_tag(x, 'CITY'))\n",
    "    \n",
    "    \n",
    "    # TRY DIFFERENT RANDOM STATES\n",
    "    seed = random.randint(0,9999)\n",
    "    ambi_tags_shuffle = ambi_tags_shuffle.sample(frac=1.0, replace=False,  random_state=seed)\n",
    "\n",
    "    ambi_tags_shuffle['NAME cumsum'] = ambi_tags_shuffle['NAME count'].cumsum()\n",
    "    ambi_tags_shuffle['STREET cumsum'] = ambi_tags_shuffle['STREET count'].cumsum()\n",
    "    ambi_tags_shuffle['CITY cumsum'] = ambi_tags_shuffle['CITY count'].cumsum()\n",
    "\n",
    "\n",
    "    ambi_tags_shuffle['NAME norm'] = ambi_tags_shuffle['NAME cumsum']/list(ambi_tags_shuffle['NAME cumsum'])[-1]\n",
    "    ambi_tags_shuffle['STREET norm'] = ambi_tags_shuffle['STREET cumsum']/list(ambi_tags_shuffle['STREET cumsum'])[-1]\n",
    "    ambi_tags_shuffle['CITY norm'] = ambi_tags_shuffle['CITY cumsum']/list(ambi_tags_shuffle['CITY cumsum'])[-1]\n",
    "\n",
    "\n",
    "    #max_80 = max(len(single_tags_shuffle.query('`NAME norm` <= 0.80')),len(single_tags_shuffle.query('`STREET norm` <= 0.80')),len(single_tags_shuffle.query('`CITY norm` <= 0.80')))\n",
    "    #min_80 = min(len(single_tags_shuffle.query('`NAME norm` <= 0.80')),len(single_tags_shuffle.query('`STREET norm` <= 0.80')),len(single_tags_shuffle.query('`CITY norm` <= 0.80')))\n",
    "    #max_90 = max(len(single_tags_shuffle.query('`NAME norm` <= 0.90')),len(single_tags_shuffle.query('`STREET norm` <= 0.90')),len(single_tags_shuffle.query('`CITY norm` <= 0.90')))\n",
    "    #min_90 = min(len(single_tags_shuffle.query('`NAME norm` <= 0.90')),len(single_tags_shuffle.query('`STREET norm` <= 0.90')),len(single_tags_shuffle.query('`CITY norm` <= 0.90')))\n",
    "    \n",
    "    max_500 = max(len(ambi_tags_shuffle.query('`NAME norm` <= 0.027289597205545246')),len(ambi_tags_shuffle.query('`STREET norm` <= 0.027289597205545246')),len(ambi_tags_shuffle.query('`CITY norm` <= 0.027289597205545246')))\n",
    "    min_500 = min(len(ambi_tags_shuffle.query('`NAME norm` <= 0.027289597205545246')),len(ambi_tags_shuffle.query('`STREET norm` <= 0.027289597205545246')),len(ambi_tags_shuffle.query('`CITY norm` <= 0.027289597205545246')))\n",
    "    \n",
    "    max_1000 = max(len(ambi_tags_shuffle.query('`NAME norm` <= 0.054457332679845344')),len(ambi_tags_shuffle.query('`STREET norm` <= 0.054457332679845344')),len(ambi_tags_shuffle.query('`CITY norm` <= 0.054457332679845344')))\n",
    "    min_1000 = min(len(ambi_tags_shuffle.query('`NAME norm` <= 0.054457332679845344')),len(ambi_tags_shuffle.query('`STREET norm` <= 0.054457332679845344')),len(ambi_tags_shuffle.query('`CITY norm` <= 0.054457332679845344')))\n",
    "    \n",
    "    dist = (max_500-min_500)+(max_1000-min_1000)\n",
    "    #dist = (max_2-min_2)\n",
    "    \n",
    "    dists.append(dist)\n",
    "    seeds.append(seed)\n",
    "\n",
    "min_index = np.argmin(dists)\n",
    "min_seed = seeds[min_index]\n",
    "for i in range(len(seeds)):\n",
    "    print(seeds[i],':',dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose best labeleling sample\n",
    "ambi_tags_shuffle = ambi_tags.copy(deep=True)\n",
    "ambi_tags_shuffle = ambi_tags_shuffle.sample(frac=1.0, replace=False,  random_state=min_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label single tags\n",
    "val_test_count = 500#math.ceil(len(ambi_tags_shuffle)*0.027289597205545246)\n",
    "train_count = len(ambi_tags_shuffle)-2*val_test_count\n",
    "\n",
    "split = train_count*['nothing']+val_test_count*['val']+val_test_count*['test']\n",
    "\n",
    "ambi_tags_shuffle['set'] = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ambi_tags_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ambi_tags_shuffle.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ambi_tags_shuffle.set.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export validation and test sets for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_set_ids = ambi_tags_shuffle.loc[ambi_tags_shuffle.set=='val','ID'].to_list()+single_tags_shuffle.loc[single_tags_shuffle.set=='val','ID'].to_list()+no_tags_shuffle.loc[no_tags_shuffle.set=='val','ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_ids = ambi_tags_shuffle.loc[ambi_tags_shuffle.set=='test','ID'].to_list()+single_tags_shuffle.loc[single_tags_shuffle.set=='test','ID'].to_list()+no_tags_shuffle.loc[no_tags_shuffle.set=='test','ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_set_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(test_set_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(val_set_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_set_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(int(1500/20)):\n",
    "    with open('Annotate/val_'+str(i*20)+'_'+str(i*20+19)+'.txt','w') as file:\n",
    "        for index in text.loc[text.ID.isin(val_set_ids[i*20:i*20+20])].index:\n",
    "            _ = file.write(text.Samples.at[index]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(1500/20)):\n",
    "    with open('Annotate/test_'+str(i*20)+'_'+str(i*20+19)+'.txt','w') as file:\n",
    "        for index in text.loc[text.ID.isin(test_set_ids[i*20:i*20+20])].index:\n",
    "            _ = file.write(text.Samples.at[index]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
