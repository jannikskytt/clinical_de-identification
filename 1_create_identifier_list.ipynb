{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">1. Create identifier list</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import timeit\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show all outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Progress bar\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Danish orthographic dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compare all names to a list of Danish words to identify ambiguous names and remove them from list\n",
    "#Load dictionary\n",
    "words = pd.read_csv('Databases/RO2012 fuldformer 2019.txt', sep=';', comment='#', keep_default_na=False, usecols=[1], names=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean\n",
    "words['word'] = words['word'].str.lower()\n",
    "words.drop_duplicates(subset=['word'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compare all names to a list of Danish words to identify ambiguous names and remove them from list\n",
    "#Load dictionary\n",
    "drugs = pd.read_excel('Databases/ListeOverGodkendteLaegemidler.xlsx', sheet_name=0, keep_default_na=False, comment='#', usecols=[1], names=['drug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning: Removing empty lines\n",
    "drugs.drop([13783, 13784], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lowercase and drop duplicates\n",
    "drugs['drug'] = drugs['drug'].str.lower()\n",
    "drugs.drop_duplicates(subset=['drug'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load eponyms\n",
    "abb = pd.read_excel('Databases/Abbreviations.xlsx', sheet_name=\"All appended\", header=None)\n",
    "abb.columns = ['abb']\n",
    "abb['abb'] = abb['abb'].str.lower()\n",
    "abb.drop_duplicates(subset=['abb'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNOMED CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load snomed descriptions\n",
    "snomed = pd.read_csv('Databases/sct2_Description_Snapshot-da_DK1000005_20200930.txt', sep='\\t', keep_default_na=False, quoting=3, usecols=['term'])\n",
    "snomed = snomed.append(pd.read_csv('Databases/sct2_Description_Snapshot-en_DK1000005_20200930.txt', sep='\\t', keep_default_na=False, quoting=3, usecols=['term']))\n",
    "snomed = snomed.append(pd.read_csv('Databases/sct2_Description_Snapshot-en_INT_20200731.txt', sep='\\t', keep_default_na=False, quoting=3, usecols=['term']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "snomed['term'] = snomed['term'].str.lower()\n",
    "snomed.drop_duplicates(subset=['term'], inplace=True)\n",
    "snomed.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Danish healthcare system’s classification system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load sks descriptions\n",
    "sks = pd.read_fwf('Databases/SKScomplete.txt', encoding='cp1252', widths=[3,20,8,8,8,120,3,3,3,3,3,1,2,2,1,25,1], usecols=[5], names=['kodetekst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "sks['kodetekst'] = sks['kodetekst'].str.lower()\n",
    "sks.drop_duplicates(subset=['kodetekst'], inplace=True)\n",
    "sks.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifiers and rate of occurrence in population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load last names\n",
    "last = pd.read_csv('Databases/efternavne 2021.txt', encoding='cp1252', comment='#', keep_default_na=False, sep='\\t', usecols=[0,1], names=['entity','count'], dtype={'entity': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning: Removing 000, 'Ej efternavn', and empty names\n",
    "last.drop([35, 532, 905, 19884, 87391, 195990, 195997, 216509, 286667], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Male names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load male names\n",
    "male = pd.read_csv('Databases/fornavne 2021 - mænd.txt', encoding='cp1252', comment='#', keep_default_na=False, sep='\\t', usecols=[0,1], names=['entity','count'], dtype={'entity': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning: Removing 000, and empty names\n",
    "male.drop([545, 102, 58924], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Female names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load female names\n",
    "female = pd.read_csv('Databases/fornavne 2021 - kvinder.txt', encoding='cp1252', comment='#', keep_default_na=False, sep='\\t', usecols=[0,1], names=['entity','count'], dtype={'entity': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning: Removing 000, and empty names\n",
    "female.drop([654, 121, 28485], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [last, male, female]\n",
    "names = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "names['entity'] = names['entity'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicated and adding their count\n",
    "names = names.groupby(['entity'], dropna=False)['count'].apply(sum).reset_index(name='count').sort_values(by='count',ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing single-letter names\n",
    "#names = names[names['entity'].str.len()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_total = names['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names['prob_pop']=names['count']/names_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names['tag'] = 'NAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['vejnavn', 'adresseringsvejnavn', 'supplerendebynavn', 'postnrnavn', 'kommunenavn', 'regionsnavn',  'landsdelsnavn']\n",
    "cities_streets = pd.read_csv('Databases/adresser.csv', sep=',', usecols=use_cols, keep_default_na=False, na_values='', dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets['vejnavn'] = cities_streets['vejnavn'].str.lower()\n",
    "cities_streets['adresseringsvejnavn'] = cities_streets['adresseringsvejnavn'].str.lower()\n",
    "cities_streets['supplerendebynavn'] = cities_streets['supplerendebynavn'].str.lower()\n",
    "cities_streets['postnrnavn'] = cities_streets['postnrnavn'].str.lower()\n",
    "cities_streets['kommunenavn'] = cities_streets['kommunenavn'].str.lower()\n",
    "cities_streets['regionsnavn'] = cities_streets['regionsnavn'].str.lower()\n",
    "cities_streets['landsdelsnavn'] = cities_streets['landsdelsnavn'].str.lower()\n",
    "cities_streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of duplicates per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets.loc[(cities_streets['supplerendebynavn']==cities_streets['postnrnavn']) | (cities_streets['supplerendebynavn']==cities_streets['kommunenavn']),'supplerendebynavn'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets.loc[cities_streets['postnrnavn']==cities_streets['kommunenavn'],'postnrnavn'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets.loc[cities_streets['kommunenavn']==cities_streets['landsdelsnavn'],'kommunenavn'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cities_streets = len(cities_streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplerendebynavn = pd.DataFrame(cities_streets.groupby(['supplerendebynavn'], dropna=True).size())\n",
    "supplerendebynavn.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postnrnavn = pd.DataFrame(cities_streets.groupby(['postnrnavn'], dropna=True).size())\n",
    "postnrnavn.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kommunenavn = pd.DataFrame(cities_streets.groupby(['kommunenavn'], dropna=True).size())\n",
    "kommunenavn.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionsnavn = pd.DataFrame(cities_streets.groupby(['regionsnavn'], dropna=True).size())\n",
    "regionsnavn.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsdelsnavn = pd.DataFrame(cities_streets.groupby(['landsdelsnavn'], dropna=True).size())\n",
    "landsdelsnavn.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [supplerendebynavn, postnrnavn, kommunenavn, regionsnavn, landsdelsnavn]\n",
    "cities = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.reset_index(inplace=True)\n",
    "cities.rename({'index': 'entity'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We groupby and add here because København as a municipality will appear x times and københavn as city y times = x+y\n",
    "cities = cities.groupby(['entity'], dropna=False)['count'].apply(sum).reset_index(name='count').sort_values(by='count',ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities['prob_pop'] = cities['count']/total_cities_streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities['tag'] = 'CITY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets.loc[cities_streets['vejnavn']==cities_streets['adresseringsvejnavn'],'vejnavn'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vejnavn = pd.DataFrame(cities_streets.groupby(['vejnavn'], dropna=True).size())\n",
    "vejnavn.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adresseringsvejnavn = pd.DataFrame(cities_streets.groupby(['adresseringsvejnavn'], dropna=True).size())\n",
    "adresseringsvejnavn.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [vejnavn, adresseringsvejnavn]\n",
    "streets = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets.reset_index(inplace=True)\n",
    "streets.rename({'index': 'entity'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We groupby and add here because København as a municipality will appear x times and københavn as city y times = x+y\n",
    "streets = streets.groupby(['entity'], dropna=False)['count'].apply(sum).reset_index(name='count').sort_values(by='count',ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets['prob_pop'] = streets['count']/total_cities_streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets['tag'] = 'STREET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append all identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [names, cities, streets]\n",
    "entities = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.drop(labels='count', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.sort_values(by='prob_pop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ambiguous identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguous with multiple types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the duplicate entities to ambiguous\n",
    "ambiguous = entities.loc[entities.duplicated(subset='entity', keep=False),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous.sort_values(by='entity',inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by entity\n",
    "tag_series = ambiguous.groupby(['entity'], dropna=False)['tag'].apply(list).reset_index(name='tag')['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by entity\n",
    "ambiguous = ambiguous.groupby(['entity'], dropna=False)['prob_pop'].apply(sum).reset_index(name='prob_pop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous['tag'] = tag_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete from entities\n",
    "entities.drop_duplicates(subset='entity', keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['tag'] = entities['tag'].progress_apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCAT AMBIGUOUS AND ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [entities, ambiguous]\n",
    "all_tags = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags.sort_values(by='prob_pop',inplace=False, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguous with non-identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trie():\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n",
    "    \n",
    "    Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n",
    "    The corresponding Regex should match much faster than a simple Regex union.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def add(self, word):\n",
    "        ref = self.data\n",
    "        for char in word:\n",
    "            ref[char] = char in ref and ref[char] or {}\n",
    "            ref = ref[char]\n",
    "        ref[''] = 1\n",
    "\n",
    "    def dump(self):\n",
    "        return self.data\n",
    "\n",
    "    def quote(self, char):\n",
    "        return re.escape(char)\n",
    "\n",
    "    def _pattern(self, pData):\n",
    "        data = pData\n",
    "        if \"\" in data and len(data.keys()) == 1:\n",
    "            return None\n",
    "\n",
    "        alt = []\n",
    "        cc = []\n",
    "        q = 0\n",
    "        for char in sorted(data.keys()):\n",
    "            if isinstance(data[char], dict):\n",
    "                try:\n",
    "                    recurse = self._pattern(data[char])\n",
    "                    alt.append(self.quote(char) + recurse)\n",
    "                except:\n",
    "                    cc.append(self.quote(char))\n",
    "            else:\n",
    "                q = 1\n",
    "        cconly = not len(alt) > 0\n",
    "\n",
    "        if len(cc) > 0:\n",
    "            if len(cc) == 1:\n",
    "                alt.append(cc[0])\n",
    "            else:\n",
    "                alt.append('[' + ''.join(cc) + ']')\n",
    "\n",
    "        if len(alt) == 1:\n",
    "            result = alt[0]\n",
    "        else:\n",
    "            result = \"(?:\" + \"|\".join(alt) + \")\"\n",
    "\n",
    "        if q:\n",
    "            if cconly:\n",
    "                result += \"?\"\n",
    "            else:\n",
    "                result = \"(?:%s)?\" % result\n",
    "        return result\n",
    "\n",
    "    def pattern(self):\n",
    "        return self._pattern(self.dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_screen_terminal(string):\n",
    "    now = datetime.now().strftime(\"[%d/%m/%Y %H:%M:%S]\")\n",
    "    print(now+\" \"+string)\n",
    "    write = os.write(1, bytes(now+\" \"+string+\"\\n\", 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAmbi(names_series, ambi_series):\n",
    "    '''\n",
    "    Takes pandas series of names and ambiguous names and returns dataframe of positive matches with column of ambiguous examples for each row.\n",
    "    \n",
    "    Identifiers were also matched if they were followed by a genitive case, which for Danish is for all words not ending in s, x, or z to take a possessive ending s. Words that do end in s, x, or z take an apostrophe.\n",
    "    This introduced a problem for ambiguous identifiers only differentiated by an ending s, e.g. the names (lowercased) “han” and “hans”: the latter was always returned as the longest match even if it carried the meaning of “han” taking a possessive ending s.\n",
    "    For this reason, the trie-based regex for identifiers was split into two: one for all identifiers ending in s, x or z, and one for all other identifiers. The final regex was:\n",
    "    The regex is run twice with trie and trie_s in the equation matching identifiers ending and not ending in s, x, and z, respectively.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print_screen_terminal('Creating regex')\n",
    "    trie = Trie()\n",
    "    trie_s = Trie()\n",
    "    for key in names_series.tolist():\n",
    "        trie_s.add(key) if key.endswith('s') else trie.add(key)\n",
    "        \n",
    "    regex= re.compile(r\"(?<!\\w)\" + trie.pattern() + r\"(?:(?:(?<![szx])(?:(?!\\w)|(?=s(?!\\w))))|(?:(?<=[szx])(?!\\w)))\", re.IGNORECASE)\n",
    "    regex_s= re.compile(r\"(?<!\\w)\" + trie_s.pattern() + r\"(?:(?:(?<![szx])(?:(?!\\w)|(?=s(?!\\w))))|(?:(?<=[szx])(?!\\w)))\", re.IGNORECASE)\n",
    "    \n",
    "    frame = pd.DataFrame({'ambi': ambi_series})\n",
    "    \n",
    "    # Extract the names that occur in the example\n",
    "    print_screen_terminal('Creating column with list of entites that match ambi for each row')\n",
    "    frame['entity'] = frame['ambi'].progress_apply(lambda x: list(set(entity.lower() for entity in (regex.findall(x)+regex_s.findall(x))))) #if name appears multiple times, we lower all, and remove duplicates\n",
    "    \n",
    "    #Split those names so that they get a row each (with example exploded)\n",
    "    print_screen_terminal('Exploding the lists of entities and dropping resulting rows with nas')\n",
    "    frame = frame.explode('entity', ignore_index=True)\n",
    "    frame.dropna(axis=0, how='any', thresh=None, subset=['entity'], inplace=True)\n",
    "    \n",
    "    #Join all the examples for each name\n",
    "    print_screen_terminal('Grouping by entity and appending ambi rows that it was matched again in list')\n",
    "    frame = frame.groupby(['entity'], dropna=False)['ambi'].apply(list).reset_index(name='ambis')\n",
    "    \n",
    "    frame.to_csv('ambi_examples.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    \n",
    "    return frame['entity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_words_list = []\n",
    "print_screen_terminal('CHECKING AMBIGUOUS WORDS')\n",
    "ambiguous_words_list += findAmbi(all_tags['entity'], words['word'])\n",
    "print_screen_terminal('APPENDING AMBIGUOUS WORDS TAG TO ENTITIES')\n",
    "all_tags['tag'] = all_tags.progress_apply(lambda x: x['tag']+['WORDS'] if x['entity'] in ambiguous_words_list else x['tag'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_abb_list = []\n",
    "print_screen_terminal('CHECKING AMBIGUOUS ABBREVIATIONS')\n",
    "ambiguous_abb_list += findAmbi(all_tags['entity'], abb['abb'])\n",
    "print_screen_terminal('APPENDING AMBIGUOUS ABBREVIATIONS TAG TO ENTITIES')\n",
    "all_tags['tag'] = all_tags.progress_apply(lambda x: x['tag']+['ABB'] if x['entity'] in ambiguous_abb_list else x['tag'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_drugs_list = []\n",
    "print_screen_terminal('CHECKING AMBIGUOUS DRUGS')\n",
    "ambiguous_drugs_list += findAmbi(all_tags['entity'], drugs['drug'])\n",
    "print_screen_terminal('APPENDING AMBIGUOUS DRUGS TAG TO ENTITIES')\n",
    "all_tags['tag'] = all_tags.progress_apply(lambda x: x['tag']+['DRUGS'] if x['entity'] in ambiguous_drugs_list else x['tag'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_snomed_list = []\n",
    "print_screen_terminal('CHECKING AMBIGUOUS SNOMED TERMS')\n",
    "ambiguous_snomed_list += findAmbi(all_tags['entity'], snomed['term'])\n",
    "print_screen_terminal('APPENDING AMBIGUOUS SNOMED TERMS TAG TO ENTITIES')\n",
    "all_tags['tag'] = all_tags.progress_apply(lambda x: x['tag']+['SNOMED'] if x['entity'] in ambiguous_snomed_list else x['tag'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_sks_list = []\n",
    "print_screen_terminal('CHECKING AMBIGUOUS SKS TERMS')\n",
    "ambiguous_sks_list += findAmbi(all_tags['entity'], sks['kodetekst'])\n",
    "print_screen_terminal('APPENDING AMBIGUOUS SKS TERMS TAG TO ENTITIES')\n",
    "all_tags['tag'] = all_tags.progress_apply(lambda x: x['tag']+['SKS'] if x['entity'] in ambiguous_sks_list else x['tag'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags.sort_values(by='prob_pop',ascending=False,inplace=False, ignore_index=True).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags.to_csv('all_tags.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
